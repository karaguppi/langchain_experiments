{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW_wLiIUjMv-"
      },
      "source": [
        "# LangChain 0.1.0 Quickstart - Create Your Own RAG Chains\n",
        "\n",
        "Welcome to this Quickstart walkthrough of the new version of Langchain. This tutorial follows the official Langchain documentation quickstart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxslo5rjgTfS"
      },
      "source": [
        "# 1.0 Install the required packages\n",
        "\n",
        "Some packages have changed inside Langchain. This is how you should install and import them from now on. Remember that langchain has been divided into three main packages:\n",
        "- `langchain` contains pre-built chains and other stuff you may need.\n",
        "- `langchain-core` contains the main abstractions of the library.\n",
        "- `langchain-community` contains third-party integrations like Chroma, FAISS, etc.\n",
        "- Some third-party integrations have their own package (outside `langchain-community`) that you should install if you want to use them. For example, OpenAI has its own package: `langchain-openai`.\n",
        "\n",
        "All three are installed when you run `pip install langchain`. But you can install community or core individually as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EfpEJfXIa_f",
        "outputId": "a75027a2-3b52-4aeb-cf02-111711dc3322"
      },
      "outputs": [],
      "source": [
        "! pip install langchain\n",
        "! pip install langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "arL99aOJUyXl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_gy2VgCqVHT8"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZjTvnJPVR7H",
        "outputId": "ad678b23-ce71-4acf-8495-b9727cd5d9d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='LangSmith is a software tool that helps users to easily generate language models and deploy them for various natural language processing tasks. It provides a user-friendly interface for creating and training models using pre-built datasets or custom data. Users can fine-tune the models according to their specific requirements and deploy them for tasks such as text classification, sentiment analysis, named entity recognition, and more.\\n\\nTo use LangSmith, users can follow these steps:\\n\\n1. Install LangSmith by following the instructions provided on their website or documentation.\\n2. Create a new project and import the necessary datasets or input data.\\n3. Choose the type of language model you want to create (e.g., text classification, sentiment analysis).\\n4. Configure the model parameters and hyperparameters according to your requirements.\\n5. Train the model using the provided tools and monitor its performance.\\n6. Fine-tune the model if needed and evaluate its accuracy.\\n7. Deploy the model for your desired NLP tasks and integrate it into your applications or workflows.\\n\\nOverall, LangSmith simplifies the process of creating and deploying language models for NLP tasks, making it accessible to users with varying levels of expertise in machine learning and natural language processing.')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"what is LangSmith and How to use it?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfNQP9jbhYCW"
      },
      "source": [
        "# 2.0 Create your first chain with LCEL\n",
        "\n",
        "LCEL is now the default way to create chains in LangChain. It has a more pipeline-like syntax and allows you to modify already-existing chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sc8OnZCxYvbT"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an English-Polosh translator that return whatever the user says in Polish\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "26-oW1y_Z6hJ"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ_tWUEVZ-uA",
        "outputId": "45493a04-fbdb-4179-9c63-1cfba4af05dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Cześć, Dzień Dobry.')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\n",
        "    \"input\": \"Hello, Good Morning\"\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_xGLWivbeOf"
      },
      "outputs": [],
      "source": [
        "# add output parser to the chain\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr5pD5_-ccrY"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPF1D7XzhoN-"
      },
      "source": [
        "# 3.0 Create a Retrieval Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HziCjCdGj4X3"
      },
      "source": [
        "\n",
        "## 3.1 Load the source documents\n",
        "\n",
        "First, we will have to load the documents that will enrich our LLM prompt. We will use [this blog post](https://blog.langchain.dev/langchain-v0-1-0/) from LangChain's official website explaining the new release. OpenAI's models were not trained on this content, so the only way to ask questions about it is to build a RAG chain.\n",
        "\n",
        "The first thing to do is to load the blog content to our vector store. We will use beautiful soup to scrap the blog post. Then we will store it in a FAISS vector store. You can use any other vector store you prefer, common ones include CassandraDB or ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndg7ytihAs2r",
        "outputId": "5e18451c-af5d-48a6-9125-735c57fbb998"
      },
      "outputs": [],
      "source": [
        "# retrieval chain\n",
        "\n",
        "! pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H69g1ttYA47U",
        "outputId": "83dda3a1-ca3b-4427-ad14-ca4ad8740d8b"
      },
      "outputs": [],
      "source": [
        "! pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJBh0nZpA_jH"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://blog.langchain.dev/week-of-2-19-langchain-release-notes/\")\n",
        "\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-CjL7LPCZDa",
        "outputId": "f649b58b-6b85-4d64-9104-ac3c5f9554ab"
      },
      "outputs": [],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc3aDflhCZwk"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQER2E4bCwp1"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter  import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W7kj0RoDNzV"
      },
      "outputs": [],
      "source": [
        "vectorstore = FAISS.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtWWJQzhiqHl"
      },
      "source": [
        "## 3.2 Create a Context-Aware LLM Chain\n",
        "\n",
        "Here we create a chain that will answer a question given a context. For now, we are passing the context manually, but in the next step we will pass in the documents fetched from the vector store we created above 👆"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrf_EX26DRHO"
      },
      "outputs": [],
      "source": [
        "# create chain for documents\n",
        "\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "template = \"\"\"\"Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2caJnGH-FFV0",
        "outputId": "8552a967-5843-47a3-a96f-1f7d6c43e7fa"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "document_chain.invoke({\n",
        "    \"input\": \"what is langSmith\",\n",
        "    \"context\": [Document(page_content=\"langsmith is a debugging and testing framework used in LLM or Rag based applications.\")]\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oW_TGIBjj_c"
      },
      "source": [
        "## 3.2 Create the RAG Chain\n",
        "\n",
        "RAG stands for Retrieval-Augmented Generation. This means that we will enrich the prompt that we send to the LLM. We will use with the documents that wil will retrieve from the vector store for this. LangChain comes with the function `create_retrieval_chain` that allows you to create one of these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhvXHSkPHb0K"
      },
      "outputs": [],
      "source": [
        "# create retrieval chain\n",
        "\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtdL4bsEFrP_"
      },
      "outputs": [],
      "source": [
        "response = retrieval_chain.invoke({\n",
        "    \"input\": \"what is new in langsmith\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "NvL_yXp-I-AG",
        "outputId": "23d86a2a-b38d-45c8-d816-018993707a11"
      },
      "outputs": [],
      "source": [
        "response['answer']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKELJ-LVkE07"
      },
      "source": [
        "# 4.0 Create Conversational RAG Chain\n",
        "\n",
        "Now we will create exactly the same thing as above, but we will have the AI assistant take the history of the conversation into account. In short, we will build the same chain as above but with we will take into account the previous messages in these two steps of the chain:\n",
        "\n",
        "- When fetching the documents from the vector store. We will fetch documents related to the entire conversation and not just the latest message.\n",
        "- When answering the question. We will send to the LLM the history of the conversation along the context and query.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHkQopzYqlaC"
      },
      "source": [
        "## 4.1 Create a Conversation-Aware Retrieval Chain\n",
        "\n",
        "This chain will return the documents related to the entire conversation and not just the latest message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZmG7GY4I_XG"
      },
      "outputs": [],
      "source": [
        "# conversational retrieval chain\n",
        "\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
        "])\n",
        "\n",
        "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6zrgY-KSys4",
        "outputId": "5edefc84-6fb4-4b84-c8a2-102a615f5615"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "chat_history = [\n",
        "    HumanMessage(content=\"Is there anything new about LangSmith?\"),\n",
        "    AIMessage(content=\"Yes!\")\n",
        "]\n",
        "\n",
        "retriever_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Tell me more about it!\"\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNjXf9UNq3Wv"
      },
      "source": [
        "## 4.2 Use Retrieval Chain together with Document Chain\n",
        "\n",
        "Now we will create a document chain that contains a placeholder for the conversation history. This placeholder will be populated with the conversation history that we will pass as its value. We will the plug it together with the retriever chain we created above to have a conversational retrieval-augmented chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKOR41J8Vj6s"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "conversational_retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm_KBfo6XxkJ"
      },
      "outputs": [],
      "source": [
        "response = conversational_retrieval_chain.invoke({\n",
        "    'chat_history': [],\n",
        "    \"input\": \"What is langsmith?\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOWw4-FLX-i0",
        "outputId": "f2a9a902-52b7-4ddc-d8a1-89516a56b1e9"
      },
      "outputs": [],
      "source": [
        "response\n",
        "# we only want the answer from the response so we can tap into the list and get the response answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYRTU-XpYJC3"
      },
      "outputs": [],
      "source": [
        "# simulate conversation history\n",
        "\n",
        "chat_history = [\n",
        "    HumanMessage(content=\"Is there anything new about LangSmith?\"),\n",
        "    AIMessage(content=\"Yes!\")\n",
        "]\n",
        "\n",
        "response = conversational_retrieval_chain.invoke({\n",
        "    'chat_history': chat_history,\n",
        "    \"input\": \"Tell me more about it!\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "FI2NB_yjYc_4",
        "outputId": "5b8a2153-2703-4431-c744-766901cbeab0"
      },
      "outputs": [],
      "source": [
        "response['answer']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7jzZ_DEsPH5"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Congratulations for finishing this tutorial. For a more in-depth walk-through, you can go to the official [LangChain documentation](https://python.langchain.com/docs/get_started/quickstart)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
